{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas MultiFADL One-versus-Rest. Brain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from src import utils\n",
    "\n",
    "from src.MonoFADLModel import MonoFADLModel\n",
    "from src.MultiFADLModelOvR import MultiFADLModelOvR\n",
    "from src.NoSelectionModel import NoSelectionModel\n",
    "\n",
    "# Seed for neural network executions\n",
    "SEED = 1234\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_results = pd.DataFrame(columns=['test_loss', 'test_accuracy', 'test_f1', 'number of selected features', 'selected Features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Ec-bioD-3_at</th>\n",
       "      <th>AFFX-r2-Ec-bioD-5_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.498150</td>\n",
       "      <td>7.604868</td>\n",
       "      <td>6.880934</td>\n",
       "      <td>9.027128</td>\n",
       "      <td>4.176175</td>\n",
       "      <td>7.224920</td>\n",
       "      <td>6.085942</td>\n",
       "      <td>6.835999</td>\n",
       "      <td>5.898355</td>\n",
       "      <td>...</td>\n",
       "      <td>9.979005</td>\n",
       "      <td>9.926470</td>\n",
       "      <td>12.719785</td>\n",
       "      <td>12.777792</td>\n",
       "      <td>5.403657</td>\n",
       "      <td>4.870548</td>\n",
       "      <td>4.047380</td>\n",
       "      <td>3.721936</td>\n",
       "      <td>4.516434</td>\n",
       "      <td>4.749940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.067436</td>\n",
       "      <td>7.998090</td>\n",
       "      <td>7.209076</td>\n",
       "      <td>9.723322</td>\n",
       "      <td>4.826126</td>\n",
       "      <td>7.539381</td>\n",
       "      <td>6.250962</td>\n",
       "      <td>8.012549</td>\n",
       "      <td>5.453147</td>\n",
       "      <td>...</td>\n",
       "      <td>11.924749</td>\n",
       "      <td>11.215930</td>\n",
       "      <td>13.605662</td>\n",
       "      <td>13.401342</td>\n",
       "      <td>5.224555</td>\n",
       "      <td>4.895315</td>\n",
       "      <td>3.786437</td>\n",
       "      <td>3.564481</td>\n",
       "      <td>4.430891</td>\n",
       "      <td>4.491416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.068179</td>\n",
       "      <td>8.573674</td>\n",
       "      <td>8.647684</td>\n",
       "      <td>9.613002</td>\n",
       "      <td>4.396581</td>\n",
       "      <td>7.813101</td>\n",
       "      <td>6.007746</td>\n",
       "      <td>7.178156</td>\n",
       "      <td>8.400266</td>\n",
       "      <td>...</td>\n",
       "      <td>12.154405</td>\n",
       "      <td>11.532460</td>\n",
       "      <td>13.764593</td>\n",
       "      <td>13.477800</td>\n",
       "      <td>5.303565</td>\n",
       "      <td>5.052184</td>\n",
       "      <td>4.005343</td>\n",
       "      <td>3.595382</td>\n",
       "      <td>4.563494</td>\n",
       "      <td>4.668827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12.456040</td>\n",
       "      <td>9.098977</td>\n",
       "      <td>6.628784</td>\n",
       "      <td>8.517677</td>\n",
       "      <td>4.154847</td>\n",
       "      <td>8.361843</td>\n",
       "      <td>6.596064</td>\n",
       "      <td>6.347285</td>\n",
       "      <td>4.900380</td>\n",
       "      <td>...</td>\n",
       "      <td>11.969072</td>\n",
       "      <td>11.288801</td>\n",
       "      <td>13.600828</td>\n",
       "      <td>13.379029</td>\n",
       "      <td>4.953429</td>\n",
       "      <td>4.708371</td>\n",
       "      <td>3.892318</td>\n",
       "      <td>3.759429</td>\n",
       "      <td>4.748381</td>\n",
       "      <td>4.521275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>12.699958</td>\n",
       "      <td>8.800721</td>\n",
       "      <td>11.556188</td>\n",
       "      <td>9.166309</td>\n",
       "      <td>4.165891</td>\n",
       "      <td>7.923826</td>\n",
       "      <td>6.212754</td>\n",
       "      <td>6.866387</td>\n",
       "      <td>5.405628</td>\n",
       "      <td>...</td>\n",
       "      <td>11.411701</td>\n",
       "      <td>11.169317</td>\n",
       "      <td>13.751442</td>\n",
       "      <td>13.803646</td>\n",
       "      <td>4.892677</td>\n",
       "      <td>4.773806</td>\n",
       "      <td>3.796856</td>\n",
       "      <td>3.577544</td>\n",
       "      <td>4.504385</td>\n",
       "      <td>4.541450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4</td>\n",
       "      <td>12.658228</td>\n",
       "      <td>8.843270</td>\n",
       "      <td>7.672655</td>\n",
       "      <td>9.125912</td>\n",
       "      <td>5.495477</td>\n",
       "      <td>8.603892</td>\n",
       "      <td>7.747514</td>\n",
       "      <td>5.828978</td>\n",
       "      <td>6.926720</td>\n",
       "      <td>...</td>\n",
       "      <td>13.170441</td>\n",
       "      <td>12.676080</td>\n",
       "      <td>14.124837</td>\n",
       "      <td>13.996436</td>\n",
       "      <td>4.913579</td>\n",
       "      <td>4.399176</td>\n",
       "      <td>3.878855</td>\n",
       "      <td>3.680103</td>\n",
       "      <td>4.726784</td>\n",
       "      <td>4.564637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>4</td>\n",
       "      <td>12.812823</td>\n",
       "      <td>8.510550</td>\n",
       "      <td>8.729699</td>\n",
       "      <td>9.104402</td>\n",
       "      <td>3.967228</td>\n",
       "      <td>7.719089</td>\n",
       "      <td>7.092496</td>\n",
       "      <td>6.504812</td>\n",
       "      <td>6.157163</td>\n",
       "      <td>...</td>\n",
       "      <td>13.040267</td>\n",
       "      <td>12.403316</td>\n",
       "      <td>13.978009</td>\n",
       "      <td>13.812916</td>\n",
       "      <td>5.189600</td>\n",
       "      <td>4.912618</td>\n",
       "      <td>3.764800</td>\n",
       "      <td>3.664920</td>\n",
       "      <td>4.628355</td>\n",
       "      <td>4.761351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4</td>\n",
       "      <td>12.706991</td>\n",
       "      <td>8.795721</td>\n",
       "      <td>7.772359</td>\n",
       "      <td>8.327273</td>\n",
       "      <td>6.329383</td>\n",
       "      <td>8.550471</td>\n",
       "      <td>6.613332</td>\n",
       "      <td>6.308945</td>\n",
       "      <td>7.494852</td>\n",
       "      <td>...</td>\n",
       "      <td>12.825383</td>\n",
       "      <td>12.439265</td>\n",
       "      <td>14.328373</td>\n",
       "      <td>14.008693</td>\n",
       "      <td>4.931460</td>\n",
       "      <td>4.712895</td>\n",
       "      <td>3.913637</td>\n",
       "      <td>3.700964</td>\n",
       "      <td>4.764693</td>\n",
       "      <td>4.834952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4</td>\n",
       "      <td>12.684593</td>\n",
       "      <td>8.293938</td>\n",
       "      <td>7.228186</td>\n",
       "      <td>8.494428</td>\n",
       "      <td>6.049414</td>\n",
       "      <td>8.214729</td>\n",
       "      <td>7.287758</td>\n",
       "      <td>5.732710</td>\n",
       "      <td>6.296021</td>\n",
       "      <td>...</td>\n",
       "      <td>13.116581</td>\n",
       "      <td>12.657967</td>\n",
       "      <td>14.390346</td>\n",
       "      <td>14.194904</td>\n",
       "      <td>4.871092</td>\n",
       "      <td>4.739400</td>\n",
       "      <td>3.782980</td>\n",
       "      <td>3.920363</td>\n",
       "      <td>4.665584</td>\n",
       "      <td>4.613326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4</td>\n",
       "      <td>12.397722</td>\n",
       "      <td>8.843524</td>\n",
       "      <td>8.825100</td>\n",
       "      <td>8.551541</td>\n",
       "      <td>5.002072</td>\n",
       "      <td>8.547894</td>\n",
       "      <td>6.920827</td>\n",
       "      <td>5.738159</td>\n",
       "      <td>6.383309</td>\n",
       "      <td>...</td>\n",
       "      <td>13.117180</td>\n",
       "      <td>12.699932</td>\n",
       "      <td>14.286779</td>\n",
       "      <td>14.088950</td>\n",
       "      <td>4.932377</td>\n",
       "      <td>4.700205</td>\n",
       "      <td>3.952850</td>\n",
       "      <td>3.678112</td>\n",
       "      <td>5.018501</td>\n",
       "      <td>4.704779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 54676 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type  1007_s_at   1053_at     117_at    121_at  1255_g_at   1294_at  \\\n",
       "0       1  12.498150  7.604868   6.880934  9.027128   4.176175  7.224920   \n",
       "1       1  13.067436  7.998090   7.209076  9.723322   4.826126  7.539381   \n",
       "2       1  13.068179  8.573674   8.647684  9.613002   4.396581  7.813101   \n",
       "3       1  12.456040  9.098977   6.628784  8.517677   4.154847  8.361843   \n",
       "4       1  12.699958  8.800721  11.556188  9.166309   4.165891  7.923826   \n",
       "..    ...        ...       ...        ...       ...        ...       ...   \n",
       "125     4  12.658228  8.843270   7.672655  9.125912   5.495477  8.603892   \n",
       "126     4  12.812823  8.510550   8.729699  9.104402   3.967228  7.719089   \n",
       "127     4  12.706991  8.795721   7.772359  8.327273   6.329383  8.550471   \n",
       "128     4  12.684593  8.293938   7.228186  8.494428   6.049414  8.214729   \n",
       "129     4  12.397722  8.843524   8.825100  8.551541   5.002072  8.547894   \n",
       "\n",
       "      1316_at   1320_at  1405_i_at  ...  AFFX-r2-Ec-bioD-3_at  \\\n",
       "0    6.085942  6.835999   5.898355  ...              9.979005   \n",
       "1    6.250962  8.012549   5.453147  ...             11.924749   \n",
       "2    6.007746  7.178156   8.400266  ...             12.154405   \n",
       "3    6.596064  6.347285   4.900380  ...             11.969072   \n",
       "4    6.212754  6.866387   5.405628  ...             11.411701   \n",
       "..        ...       ...        ...  ...                   ...   \n",
       "125  7.747514  5.828978   6.926720  ...             13.170441   \n",
       "126  7.092496  6.504812   6.157163  ...             13.040267   \n",
       "127  6.613332  6.308945   7.494852  ...             12.825383   \n",
       "128  7.287758  5.732710   6.296021  ...             13.116581   \n",
       "129  6.920827  5.738159   6.383309  ...             13.117180   \n",
       "\n",
       "     AFFX-r2-Ec-bioD-5_at  AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  \\\n",
       "0                9.926470            12.719785            12.777792   \n",
       "1               11.215930            13.605662            13.401342   \n",
       "2               11.532460            13.764593            13.477800   \n",
       "3               11.288801            13.600828            13.379029   \n",
       "4               11.169317            13.751442            13.803646   \n",
       "..                    ...                  ...                  ...   \n",
       "125             12.676080            14.124837            13.996436   \n",
       "126             12.403316            13.978009            13.812916   \n",
       "127             12.439265            14.328373            14.008693   \n",
       "128             12.657967            14.390346            14.194904   \n",
       "129             12.699932            14.286779            14.088950   \n",
       "\n",
       "     AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  AFFX-TrpnX-3_at  \\\n",
       "0          5.403657        4.870548        4.047380         3.721936   \n",
       "1          5.224555        4.895315        3.786437         3.564481   \n",
       "2          5.303565        5.052184        4.005343         3.595382   \n",
       "3          4.953429        4.708371        3.892318         3.759429   \n",
       "4          4.892677        4.773806        3.796856         3.577544   \n",
       "..              ...             ...             ...              ...   \n",
       "125        4.913579        4.399176        3.878855         3.680103   \n",
       "126        5.189600        4.912618        3.764800         3.664920   \n",
       "127        4.931460        4.712895        3.913637         3.700964   \n",
       "128        4.871092        4.739400        3.782980         3.920363   \n",
       "129        4.932377        4.700205        3.952850         3.678112   \n",
       "\n",
       "     AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "0           4.516434         4.749940  \n",
       "1           4.430891         4.491416  \n",
       "2           4.563494         4.668827  \n",
       "3           4.748381         4.521275  \n",
       "4           4.504385         4.541450  \n",
       "..               ...              ...  \n",
       "125         4.726784         4.564637  \n",
       "126         4.628355         4.761351  \n",
       "127         4.764693         4.834952  \n",
       "128         4.665584         4.613326  \n",
       "129         5.018501         4.704779  \n",
       "\n",
       "[130 rows x 54676 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset and preprocess it\n",
    "# https://www.kaggle.com/datasets/brunogrisci/brain-cancer-gene-expression-cumida\n",
    "\n",
    "# Check if stored, otherwise load and store\n",
    "try:\n",
    "    brain = pickle.load(open('data/brain_GSE50161.pkl', 'rb'))\n",
    "except:\n",
    "    brain = pd.read_csv('data/brain_GSE50161.csv')\n",
    "    brain = brain.drop(columns=['samples'])\n",
    "\n",
    "    brain['type'] = brain['type'].map({\n",
    "        'normal': 0,\n",
    "        'ependymoma': 1,\n",
    "        'glioblastoma': 2,\n",
    "        'medulloblastoma': 3,\n",
    "        'pilocytic_astrocytoma': 4\n",
    "    })\n",
    "\n",
    "    with open('data/brain_GSE50161.pkl', 'wb') as f:\n",
    "        pickle.dump(brain, f)\n",
    "        \n",
    "brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in train:\n",
      "1    28\n",
      "2    20\n",
      "3    14\n",
      "4     9\n",
      "0     7\n",
      "Name: type, dtype: int64\n",
      "Class distribution in val:\n",
      "1    9\n",
      "2    7\n",
      "3    4\n",
      "4    3\n",
      "0    3\n",
      "Name: type, dtype: int64\n",
      "Class distribution in test:\n",
      "1    9\n",
      "2    7\n",
      "3    4\n",
      "4    3\n",
      "0    3\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stratify train-val-test split\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = utils.train_test_split_stratify(df=brain, target='type', SEED=1223)\n",
    "\n",
    "# Standardization of continuous variables\n",
    "X_train_scaled, X_val_scaled, X_test_scaled = utils.scale_numerical_variables(X_train, X_test, X_val, numerical_variables = X_train.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>1431_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Ec-bioD-3_at</th>\n",
       "      <th>AFFX-r2-Ec-bioD-5_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.682064</td>\n",
       "      <td>0.812080</td>\n",
       "      <td>0.345572</td>\n",
       "      <td>0.173856</td>\n",
       "      <td>0.282813</td>\n",
       "      <td>0.684193</td>\n",
       "      <td>0.209364</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>0.580033</td>\n",
       "      <td>0.244462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667797</td>\n",
       "      <td>0.586658</td>\n",
       "      <td>0.632560</td>\n",
       "      <td>0.485589</td>\n",
       "      <td>0.478970</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.157501</td>\n",
       "      <td>0.188098</td>\n",
       "      <td>0.231972</td>\n",
       "      <td>0.374109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.830067</td>\n",
       "      <td>0.303818</td>\n",
       "      <td>0.287039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308163</td>\n",
       "      <td>0.591153</td>\n",
       "      <td>0.428715</td>\n",
       "      <td>0.484356</td>\n",
       "      <td>0.336788</td>\n",
       "      <td>0.228791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632048</td>\n",
       "      <td>0.466839</td>\n",
       "      <td>0.573824</td>\n",
       "      <td>0.490832</td>\n",
       "      <td>0.792028</td>\n",
       "      <td>0.634075</td>\n",
       "      <td>0.486110</td>\n",
       "      <td>0.163028</td>\n",
       "      <td>0.311677</td>\n",
       "      <td>0.344999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.260548</td>\n",
       "      <td>0.455918</td>\n",
       "      <td>0.195355</td>\n",
       "      <td>0.786902</td>\n",
       "      <td>0.137949</td>\n",
       "      <td>0.389816</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>0.250551</td>\n",
       "      <td>0.583605</td>\n",
       "      <td>0.219713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729421</td>\n",
       "      <td>0.619234</td>\n",
       "      <td>0.542287</td>\n",
       "      <td>0.547508</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.480383</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>0.240648</td>\n",
       "      <td>0.235842</td>\n",
       "      <td>0.379113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.848069</td>\n",
       "      <td>0.562739</td>\n",
       "      <td>0.230145</td>\n",
       "      <td>0.189416</td>\n",
       "      <td>0.805475</td>\n",
       "      <td>0.857822</td>\n",
       "      <td>0.422318</td>\n",
       "      <td>0.142876</td>\n",
       "      <td>0.162996</td>\n",
       "      <td>0.252044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721874</td>\n",
       "      <td>0.587323</td>\n",
       "      <td>0.670944</td>\n",
       "      <td>0.606149</td>\n",
       "      <td>0.466806</td>\n",
       "      <td>0.406887</td>\n",
       "      <td>0.550732</td>\n",
       "      <td>0.565437</td>\n",
       "      <td>0.411383</td>\n",
       "      <td>0.363487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.586030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119687</td>\n",
       "      <td>0.265315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.231910</td>\n",
       "      <td>0.044782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886642</td>\n",
       "      <td>0.844591</td>\n",
       "      <td>0.912946</td>\n",
       "      <td>0.935962</td>\n",
       "      <td>0.266287</td>\n",
       "      <td>0.166346</td>\n",
       "      <td>0.586971</td>\n",
       "      <td>0.201981</td>\n",
       "      <td>0.478446</td>\n",
       "      <td>0.377520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.857154</td>\n",
       "      <td>0.397044</td>\n",
       "      <td>0.240424</td>\n",
       "      <td>0.948545</td>\n",
       "      <td>0.255932</td>\n",
       "      <td>0.886523</td>\n",
       "      <td>0.234805</td>\n",
       "      <td>0.385394</td>\n",
       "      <td>0.263701</td>\n",
       "      <td>0.236133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563251</td>\n",
       "      <td>0.457280</td>\n",
       "      <td>0.650025</td>\n",
       "      <td>0.581945</td>\n",
       "      <td>0.433880</td>\n",
       "      <td>0.582573</td>\n",
       "      <td>0.337202</td>\n",
       "      <td>0.142356</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>0.501360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.376382</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.233811</td>\n",
       "      <td>0.762211</td>\n",
       "      <td>0.250391</td>\n",
       "      <td>0.216820</td>\n",
       "      <td>0.347135</td>\n",
       "      <td>0.554963</td>\n",
       "      <td>0.110032</td>\n",
       "      <td>0.198212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637219</td>\n",
       "      <td>0.531621</td>\n",
       "      <td>0.484008</td>\n",
       "      <td>0.306322</td>\n",
       "      <td>0.629953</td>\n",
       "      <td>0.355394</td>\n",
       "      <td>0.585183</td>\n",
       "      <td>0.108852</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0.527451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.680469</td>\n",
       "      <td>0.190325</td>\n",
       "      <td>0.152339</td>\n",
       "      <td>0.678392</td>\n",
       "      <td>0.288332</td>\n",
       "      <td>0.554225</td>\n",
       "      <td>0.126489</td>\n",
       "      <td>0.270648</td>\n",
       "      <td>0.610433</td>\n",
       "      <td>0.178905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097290</td>\n",
       "      <td>0.108722</td>\n",
       "      <td>0.054618</td>\n",
       "      <td>0.092563</td>\n",
       "      <td>0.481845</td>\n",
       "      <td>0.259945</td>\n",
       "      <td>0.399684</td>\n",
       "      <td>0.146089</td>\n",
       "      <td>0.177436</td>\n",
       "      <td>0.478802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.813770</td>\n",
       "      <td>0.742705</td>\n",
       "      <td>0.310691</td>\n",
       "      <td>0.592037</td>\n",
       "      <td>0.304803</td>\n",
       "      <td>0.442213</td>\n",
       "      <td>0.536939</td>\n",
       "      <td>0.356246</td>\n",
       "      <td>0.450135</td>\n",
       "      <td>0.240659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648767</td>\n",
       "      <td>0.446650</td>\n",
       "      <td>0.517752</td>\n",
       "      <td>0.512251</td>\n",
       "      <td>0.714882</td>\n",
       "      <td>0.565147</td>\n",
       "      <td>0.366649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267525</td>\n",
       "      <td>0.422152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.416596</td>\n",
       "      <td>0.438361</td>\n",
       "      <td>0.181991</td>\n",
       "      <td>0.862578</td>\n",
       "      <td>0.174371</td>\n",
       "      <td>0.176174</td>\n",
       "      <td>0.432280</td>\n",
       "      <td>0.612399</td>\n",
       "      <td>0.133926</td>\n",
       "      <td>0.172588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732279</td>\n",
       "      <td>0.599566</td>\n",
       "      <td>0.585813</td>\n",
       "      <td>0.420457</td>\n",
       "      <td>0.627784</td>\n",
       "      <td>0.361673</td>\n",
       "      <td>0.591810</td>\n",
       "      <td>0.100692</td>\n",
       "      <td>0.228607</td>\n",
       "      <td>0.474619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 54675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1007_s_at   1053_at    117_at    121_at  1255_g_at   1294_at   1316_at  \\\n",
       "75    0.682064  0.812080  0.345572  0.173856   0.282813  0.684193  0.209364   \n",
       "118   0.830067  0.303818  0.287039  1.000000   0.308163  0.591153  0.428715   \n",
       "83    0.260548  0.455918  0.195355  0.786902   0.137949  0.389816  0.338430   \n",
       "122   0.848069  0.562739  0.230145  0.189416   0.805475  0.857822  0.422318   \n",
       "92    0.149400  0.586030  0.000000  0.000000   0.119687  0.265315  1.000000   \n",
       "..         ...       ...       ...       ...        ...       ...       ...   \n",
       "117   0.857154  0.397044  0.240424  0.948545   0.255932  0.886523  0.234805   \n",
       "87    0.376382  0.535088  0.233811  0.762211   0.250391  0.216820  0.347135   \n",
       "119   0.680469  0.190325  0.152339  0.678392   0.288332  0.554225  0.126489   \n",
       "72    0.813770  0.742705  0.310691  0.592037   0.304803  0.442213  0.536939   \n",
       "84    0.416596  0.438361  0.181991  0.862578   0.174371  0.176174  0.432280   \n",
       "\n",
       "      1320_at  1405_i_at   1431_at  ...  AFFX-r2-Ec-bioD-3_at  \\\n",
       "75   0.043558   0.580033  0.244462  ...              0.667797   \n",
       "118  0.484356   0.336788  0.228791  ...              0.632048   \n",
       "83   0.250551   0.583605  0.219713  ...              0.729421   \n",
       "122  0.142876   0.162996  0.252044  ...              0.721874   \n",
       "92   0.231910   0.044782  0.000000  ...              0.886642   \n",
       "..        ...        ...       ...  ...                   ...   \n",
       "117  0.385394   0.263701  0.236133  ...              0.563251   \n",
       "87   0.554963   0.110032  0.198212  ...              0.637219   \n",
       "119  0.270648   0.610433  0.178905  ...              0.097290   \n",
       "72   0.356246   0.450135  0.240659  ...              0.648767   \n",
       "84   0.612399   0.133926  0.172588  ...              0.732279   \n",
       "\n",
       "     AFFX-r2-Ec-bioD-5_at  AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  \\\n",
       "75               0.586658             0.632560             0.485589   \n",
       "118              0.466839             0.573824             0.490832   \n",
       "83               0.619234             0.542287             0.547508   \n",
       "122              0.587323             0.670944             0.606149   \n",
       "92               0.844591             0.912946             0.935962   \n",
       "..                    ...                  ...                  ...   \n",
       "117              0.457280             0.650025             0.581945   \n",
       "87               0.531621             0.484008             0.306322   \n",
       "119              0.108722             0.054618             0.092563   \n",
       "72               0.446650             0.517752             0.512251   \n",
       "84               0.599566             0.585813             0.420457   \n",
       "\n",
       "     AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  AFFX-TrpnX-3_at  \\\n",
       "75         0.478970        0.318969        0.157501         0.188098   \n",
       "118        0.792028        0.634075        0.486110         0.163028   \n",
       "83         0.598783        0.480383        0.486800         0.240648   \n",
       "122        0.466806        0.406887        0.550732         0.565437   \n",
       "92         0.266287        0.166346        0.586971         0.201981   \n",
       "..              ...             ...             ...              ...   \n",
       "117        0.433880        0.582573        0.337202         0.142356   \n",
       "87         0.629953        0.355394        0.585183         0.108852   \n",
       "119        0.481845        0.259945        0.399684         0.146089   \n",
       "72         0.714882        0.565147        0.366649         0.000000   \n",
       "84         0.627784        0.361673        0.591810         0.100692   \n",
       "\n",
       "     AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "75          0.231972         0.374109  \n",
       "118         0.311677         0.344999  \n",
       "83          0.235842         0.379113  \n",
       "122         0.411383         0.363487  \n",
       "92          0.478446         0.377520  \n",
       "..               ...              ...  \n",
       "117         0.132800         0.501360  \n",
       "87          0.290771         0.527451  \n",
       "119         0.177436         0.478802  \n",
       "72          0.267525         0.422152  \n",
       "84          0.228607         0.474619  \n",
       "\n",
       "[78 rows x 54675 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Noselection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 80.9297 - sparse_categorical_accuracy: 0.1282 - val_loss: 24.0805 - val_sparse_categorical_accuracy: 0.2692\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 14.2374 - sparse_categorical_accuracy: 0.2436 - val_loss: 1.6052 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.8187 - sparse_categorical_accuracy: 0.3462 - val_loss: 1.6038 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5993 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5998 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5957 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5946 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5890 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5893 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 1.5830 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5836 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.5767 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5779 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 1.5717 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5723 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5650 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5678 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 1.5597 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5635 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 1.5546 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5592 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5495 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5550 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5454 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5509 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.5404 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5472 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.5366 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5437 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 1.5326 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5406 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5290 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5377 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5261 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5352 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5227 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5330 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5205 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5308 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5173 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5289 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5156 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5271 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5132 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5255 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5108 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5241 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5096 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5226 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5074 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5212 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5061 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5199 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5046 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5188 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5032 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5178 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5023 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5170 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5011 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5163 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4999 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5157 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4992 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5152 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 1.4984 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5146 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4977 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5141 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.4972 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5136 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4965 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5131 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4961 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5126 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4959 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5122 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4952 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5120 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4954 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5117 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4948 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5115 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.4947 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5114 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.4945 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5114 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4944 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5114 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4941 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5114 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.4940 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5114 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.4937 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5113 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.4935 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.5114 - val_sparse_categorical_accuracy: 0.3462\n"
     ]
    }
   ],
   "source": [
    "model1 = NoSelectionModel(\n",
    "    n_inputs=X_train_scaled.columns.values.shape[0],\n",
    "    n_class=y_train.unique().shape[0]\n",
    ")       \n",
    "\n",
    "model1.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_val_scaled,\n",
    "    y_val,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "model1.model.save('results/brain_GSE50161/brain_GSE50161_NoSelectionModel.h5')\n",
    "with open('results/brain_GSE50161/brain_GSE50161_NoSelectionModel_history.pkl', 'wb') as f:\n",
    "    pickle.dump(model1.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 1.5114 - sparse_categorical_accuracy: 0.3462\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>number of selected features</th>\n",
       "      <th>selected Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NoSelection</th>\n",
       "      <td>1.511394</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.178022</td>\n",
       "      <td>54675</td>\n",
       "      <td>[1007_s_at, 1053_at, 117_at, 121_at, 1255_g_at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             test_loss  test_accuracy   test_f1 number of selected features  \\\n",
       "NoSelection   1.511394       0.346154  0.178022                       54675   \n",
       "\n",
       "                                             selected Features  \n",
       "NoSelection  [1007_s_at, 1053_at, 117_at, 121_at, 1255_g_at...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "verbose1 = model1.get_verbose()\n",
    "\n",
    "comparative_results.loc['NoSelection'] = [verbose1['results']['loss'], \n",
    "                                          verbose1['results']['accuracy'], \n",
    "                                          verbose1['results']['f1'], \n",
    "                                          verbose1['selected_features'].shape[0], \n",
    "                                          verbose1['selected_features']]\n",
    "comparative_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MonoFADL results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 88ms/step - loss: 34.1876 - sparse_categorical_accuracy: 0.1923 - val_loss: -9.3222 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -13.5057 - sparse_categorical_accuracy: 0.3590 - val_loss: -25.6383 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -29.8144 - sparse_categorical_accuracy: 0.3590 - val_loss: -41.8887 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -46.0543 - sparse_categorical_accuracy: 0.3590 - val_loss: -58.0312 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -62.1126 - sparse_categorical_accuracy: 0.3590 - val_loss: -74.0327 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -78.1919 - sparse_categorical_accuracy: 0.3590 - val_loss: -90.1831 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -94.3201 - sparse_categorical_accuracy: 0.3590 - val_loss: -106.2626 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 74ms/step - loss: -110.3762 - sparse_categorical_accuracy: 0.3590 - val_loss: -122.2734 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -126.3715 - sparse_categorical_accuracy: 0.3590 - val_loss: -138.3568 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: -142.5410 - sparse_categorical_accuracy: 0.3590 - val_loss: -154.7336 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 132ms/step - loss: -158.9802 - sparse_categorical_accuracy: 0.3590 - val_loss: -171.2955 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -175.5668 - sparse_categorical_accuracy: 0.3590 - val_loss: -187.9111 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -192.1989 - sparse_categorical_accuracy: 0.3590 - val_loss: -204.5604 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -208.8510 - sparse_categorical_accuracy: 0.3590 - val_loss: -221.2428 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 54ms/step - loss: -225.5474 - sparse_categorical_accuracy: 0.3590 - val_loss: -237.9595 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -242.2808 - sparse_categorical_accuracy: 0.3590 - val_loss: -254.7685 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 50ms/step - loss: -259.1642 - sparse_categorical_accuracy: 0.3590 - val_loss: -271.7564 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -276.1151 - sparse_categorical_accuracy: 0.3590 - val_loss: -288.6637 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: -293.0373 - sparse_categorical_accuracy: 0.3590 - val_loss: -305.6442 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -310.0644 - sparse_categorical_accuracy: 0.3590 - val_loss: -322.7306 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -327.0506 - sparse_categorical_accuracy: 0.3590 - val_loss: -339.5109 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: -343.8254 - sparse_categorical_accuracy: 0.3590 - val_loss: -356.1469 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -360.3339 - sparse_categorical_accuracy: 0.3590 - val_loss: -372.5453 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -376.8636 - sparse_categorical_accuracy: 0.3590 - val_loss: -389.2271 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -393.4981 - sparse_categorical_accuracy: 0.3590 - val_loss: -405.7896 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -410.1213 - sparse_categorical_accuracy: 0.3590 - val_loss: -422.5091 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -426.7998 - sparse_categorical_accuracy: 0.3590 - val_loss: -439.0529 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -443.3381 - sparse_categorical_accuracy: 0.3590 - val_loss: -455.5845 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -459.8045 - sparse_categorical_accuracy: 0.3590 - val_loss: -471.9446 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -476.1606 - sparse_categorical_accuracy: 0.3590 - val_loss: -488.2498 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -492.4218 - sparse_categorical_accuracy: 0.3590 - val_loss: -504.5262 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -508.7307 - sparse_categorical_accuracy: 0.3590 - val_loss: -520.8788 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -525.1759 - sparse_categorical_accuracy: 0.3590 - val_loss: -537.3447 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -541.4987 - sparse_categorical_accuracy: 0.3590 - val_loss: -553.4888 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -557.6482 - sparse_categorical_accuracy: 0.3590 - val_loss: -569.6378 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -573.8051 - sparse_categorical_accuracy: 0.3590 - val_loss: -585.8308 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -590.0157 - sparse_categorical_accuracy: 0.3590 - val_loss: -602.0889 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -606.2892 - sparse_categorical_accuracy: 0.3590 - val_loss: -618.4043 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -622.6158 - sparse_categorical_accuracy: 0.3590 - val_loss: -634.7602 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -638.9797 - sparse_categorical_accuracy: 0.3590 - val_loss: -651.1461 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -655.3720 - sparse_categorical_accuracy: 0.3590 - val_loss: -667.5537 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -671.7833 - sparse_categorical_accuracy: 0.3590 - val_loss: -683.9760 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -688.2084 - sparse_categorical_accuracy: 0.3590 - val_loss: -700.4081 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -704.6418 - sparse_categorical_accuracy: 0.3590 - val_loss: -716.8463 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -721.0809 - sparse_categorical_accuracy: 0.3590 - val_loss: -733.2880 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -737.5225 - sparse_categorical_accuracy: 0.3590 - val_loss: -749.7313 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -753.9666 - sparse_categorical_accuracy: 0.3590 - val_loss: -766.1754 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -770.4107 - sparse_categorical_accuracy: 0.3590 - val_loss: -782.6192 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -786.8547 - sparse_categorical_accuracy: 0.3590 - val_loss: -799.0624 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -803.2975 - sparse_categorical_accuracy: 0.3590 - val_loss: -815.5043 - val_sparse_categorical_accuracy: 0.3462\n"
     ]
    }
   ],
   "source": [
    "model2 = MonoFADLModel(\n",
    "    n_inputs=X_train_scaled.columns.values.shape[0],\n",
    "    n_class=y_train.unique().shape[0]\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_val_scaled,\n",
    "    y_val,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "model2.model.save('results/brain_GSE50161/brain_GSE50161_MonoFADLModel.h5')\n",
    "with open('results/brain_GSE50161/brain_GSE50161_MonoFADLModel_history.pkl', 'wb') as f:\n",
    "    pickle.dump(model2.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: -815.5043 - sparse_categorical_accuracy: 0.3462\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>number of selected features</th>\n",
       "      <th>selected Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NoSelection</th>\n",
       "      <td>1.511394</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.178022</td>\n",
       "      <td>54675</td>\n",
       "      <td>[1007_s_at, 1053_at, 117_at, 121_at, 1255_g_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonoFADL</th>\n",
       "      <td>-815.504272</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.178022</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              test_loss  test_accuracy   test_f1 number of selected features  \\\n",
       "NoSelection    1.511394       0.346154  0.178022                       54675   \n",
       "MonoFADL    -815.504272       0.346154  0.178022                           0   \n",
       "\n",
       "                                             selected Features  \n",
       "NoSelection  [1007_s_at, 1053_at, 117_at, 121_at, 1255_g_at...  \n",
       "MonoFADL                                                    []  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "verbose2 = model2.get_verbose()\n",
    "\n",
    "comparative_results.loc['MonoFADL'] = [verbose2['results']['loss'], \n",
    "                                          verbose2['results']['accuracy'], \n",
    "                                          verbose2['results']['f1'], \n",
    "                                          verbose2['selected_features'].shape[0], \n",
    "                                          verbose2['selected_features']]\n",
    "comparative_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MultiFADL One-versus-Rest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training model class 2 vs rest\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 88ms/step - loss: 38.3934 - binary_accuracy: 0.7436 - val_loss: -10.2328 - val_binary_accuracy: 0.7308\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -14.4347 - binary_accuracy: 0.7436 - val_loss: -26.6463 - val_binary_accuracy: 0.7308\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -30.8757 - binary_accuracy: 0.7436 - val_loss: -43.1301 - val_binary_accuracy: 0.7308\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -47.3616 - binary_accuracy: 0.7436 - val_loss: -59.6497 - val_binary_accuracy: 0.7308\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -63.9124 - binary_accuracy: 0.7436 - val_loss: -76.2430 - val_binary_accuracy: 0.7308\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -80.4987 - binary_accuracy: 0.7436 - val_loss: -92.8173 - val_binary_accuracy: 0.7308\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -97.0594 - binary_accuracy: 0.7436 - val_loss: -109.3515 - val_binary_accuracy: 0.7308\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -113.5984 - binary_accuracy: 0.7436 - val_loss: -125.8684 - val_binary_accuracy: 0.7308\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -130.0751 - binary_accuracy: 0.7436 - val_loss: -142.3046 - val_binary_accuracy: 0.7308\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -146.5494 - binary_accuracy: 0.7436 - val_loss: -158.7797 - val_binary_accuracy: 0.7308\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -162.9947 - binary_accuracy: 0.7436 - val_loss: -175.1190 - val_binary_accuracy: 0.7308\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -179.2852 - binary_accuracy: 0.7436 - val_loss: -191.3009 - val_binary_accuracy: 0.7308\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 26ms/step - loss: -195.4475 - binary_accuracy: 0.7436 - val_loss: -207.3601 - val_binary_accuracy: 0.7308\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -211.4439 - binary_accuracy: 0.7436 - val_loss: -223.2402 - val_binary_accuracy: 0.7308\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: -227.3289 - binary_accuracy: 0.7436 - val_loss: -239.1185 - val_binary_accuracy: 0.7308\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -243.1848 - binary_accuracy: 0.7436 - val_loss: -254.9031 - val_binary_accuracy: 0.7308\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -258.9037 - binary_accuracy: 0.7436 - val_loss: -270.5446 - val_binary_accuracy: 0.7308\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -274.5842 - binary_accuracy: 0.7436 - val_loss: -286.2468 - val_binary_accuracy: 0.7308\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -290.2625 - binary_accuracy: 0.7436 - val_loss: -301.8848 - val_binary_accuracy: 0.7308\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: -305.8893 - binary_accuracy: 0.7436 - val_loss: -317.4816 - val_binary_accuracy: 0.7308\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -321.4711 - binary_accuracy: 0.7436 - val_loss: -333.0887 - val_binary_accuracy: 0.7308\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -337.1428 - binary_accuracy: 0.7436 - val_loss: -348.8603 - val_binary_accuracy: 0.7308\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -352.9306 - binary_accuracy: 0.7436 - val_loss: -364.6306 - val_binary_accuracy: 0.7308\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -368.6388 - binary_accuracy: 0.7436 - val_loss: -380.2602 - val_binary_accuracy: 0.7308\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -384.3312 - binary_accuracy: 0.7436 - val_loss: -395.9276 - val_binary_accuracy: 0.7308\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -399.8509 - binary_accuracy: 0.7436 - val_loss: -411.2572 - val_binary_accuracy: 0.7308\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -415.2295 - binary_accuracy: 0.7436 - val_loss: -426.7446 - val_binary_accuracy: 0.7308\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -430.6912 - binary_accuracy: 0.7436 - val_loss: -442.2117 - val_binary_accuracy: 0.7308\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -446.2376 - binary_accuracy: 0.7436 - val_loss: -457.9018 - val_binary_accuracy: 0.7308\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -461.9353 - binary_accuracy: 0.7436 - val_loss: -473.6468 - val_binary_accuracy: 0.7308\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -477.7315 - binary_accuracy: 0.7436 - val_loss: -489.5483 - val_binary_accuracy: 0.7308\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -493.6320 - binary_accuracy: 0.7436 - val_loss: -505.4519 - val_binary_accuracy: 0.7308\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -509.5362 - binary_accuracy: 0.7436 - val_loss: -521.3491 - val_binary_accuracy: 0.7308\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -525.4825 - binary_accuracy: 0.7436 - val_loss: -537.3282 - val_binary_accuracy: 0.7308\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -541.4362 - binary_accuracy: 0.7436 - val_loss: -553.2645 - val_binary_accuracy: 0.7308\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -557.3724 - binary_accuracy: 0.7436 - val_loss: -569.2090 - val_binary_accuracy: 0.7308\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -573.3134 - binary_accuracy: 0.7436 - val_loss: -585.1712 - val_binary_accuracy: 0.7308\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -589.2845 - binary_accuracy: 0.7436 - val_loss: -601.1709 - val_binary_accuracy: 0.7308\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -605.2944 - binary_accuracy: 0.7436 - val_loss: -617.2077 - val_binary_accuracy: 0.7308\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -621.3392 - binary_accuracy: 0.7436 - val_loss: -633.2734 - val_binary_accuracy: 0.7308\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -637.4109 - binary_accuracy: 0.7436 - val_loss: -649.3619 - val_binary_accuracy: 0.7308\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: -653.5044 - binary_accuracy: 0.7436 - val_loss: -665.4688 - val_binary_accuracy: 0.7308\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -669.6155 - binary_accuracy: 0.7436 - val_loss: -681.5900 - val_binary_accuracy: 0.7308\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -685.7402 - binary_accuracy: 0.7436 - val_loss: -697.7236 - val_binary_accuracy: 0.7308\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -701.8763 - binary_accuracy: 0.7436 - val_loss: -713.8671 - val_binary_accuracy: 0.7308\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -718.0222 - binary_accuracy: 0.7436 - val_loss: -730.0192 - val_binary_accuracy: 0.7308\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -734.1765 - binary_accuracy: 0.7436 - val_loss: -746.1790 - val_binary_accuracy: 0.7308\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -750.3381 - binary_accuracy: 0.7436 - val_loss: -762.3458 - val_binary_accuracy: 0.7308\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -766.5064 - binary_accuracy: 0.7436 - val_loss: -778.5184 - val_binary_accuracy: 0.7308\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -782.6806 - binary_accuracy: 0.7436 - val_loss: -794.6967 - val_binary_accuracy: 0.7308\n",
      "--> Training model class 4 vs rest\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 89ms/step - loss: 20.4181 - binary_accuracy: 0.8718 - val_loss: -10.2465 - val_binary_accuracy: 0.8846\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -14.4509 - binary_accuracy: 0.8846 - val_loss: -26.6610 - val_binary_accuracy: 0.8846\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -30.8695 - binary_accuracy: 0.8846 - val_loss: -43.0439 - val_binary_accuracy: 0.8846\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -47.2312 - binary_accuracy: 0.8846 - val_loss: -59.3652 - val_binary_accuracy: 0.8846\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -63.5474 - binary_accuracy: 0.8846 - val_loss: -75.6811 - val_binary_accuracy: 0.8846\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -79.8681 - binary_accuracy: 0.8846 - val_loss: -92.0266 - val_binary_accuracy: 0.8846\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -96.2352 - binary_accuracy: 0.8846 - val_loss: -108.4383 - val_binary_accuracy: 0.8846\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -112.6453 - binary_accuracy: 0.8846 - val_loss: -124.8482 - val_binary_accuracy: 0.8846\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -129.0557 - binary_accuracy: 0.8846 - val_loss: -141.2646 - val_binary_accuracy: 0.8846\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -145.4758 - binary_accuracy: 0.8846 - val_loss: -157.6902 - val_binary_accuracy: 0.8846\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -161.9028 - binary_accuracy: 0.8846 - val_loss: -174.1228 - val_binary_accuracy: 0.8846\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -178.3375 - binary_accuracy: 0.8846 - val_loss: -190.5626 - val_binary_accuracy: 0.8846\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -194.7797 - binary_accuracy: 0.8846 - val_loss: -207.0089 - val_binary_accuracy: 0.8846\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -211.2260 - binary_accuracy: 0.8846 - val_loss: -223.4600 - val_binary_accuracy: 0.8846\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -227.6793 - binary_accuracy: 0.8846 - val_loss: -239.9153 - val_binary_accuracy: 0.8846\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -244.1441 - binary_accuracy: 0.8846 - val_loss: -256.4037 - val_binary_accuracy: 0.8846\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -260.6296 - binary_accuracy: 0.8846 - val_loss: -272.8833 - val_binary_accuracy: 0.8846\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -277.1072 - binary_accuracy: 0.8846 - val_loss: -289.3570 - val_binary_accuracy: 0.8846\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -293.5800 - binary_accuracy: 0.8846 - val_loss: -305.8273 - val_binary_accuracy: 0.8846\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -310.0506 - binary_accuracy: 0.8846 - val_loss: -322.2953 - val_binary_accuracy: 0.8846\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -326.5181 - binary_accuracy: 0.8846 - val_loss: -338.7610 - val_binary_accuracy: 0.8846\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -342.9825 - binary_accuracy: 0.8846 - val_loss: -355.2237 - val_binary_accuracy: 0.8846\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -359.4446 - binary_accuracy: 0.8846 - val_loss: -371.6847 - val_binary_accuracy: 0.8846\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -375.8788 - binary_accuracy: 0.8846 - val_loss: -388.0553 - val_binary_accuracy: 0.8846\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -392.2622 - binary_accuracy: 0.8846 - val_loss: -404.4841 - val_binary_accuracy: 0.8846\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -408.7037 - binary_accuracy: 0.8846 - val_loss: -420.9429 - val_binary_accuracy: 0.8846\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -425.1631 - binary_accuracy: 0.8846 - val_loss: -437.4013 - val_binary_accuracy: 0.8846\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -441.6217 - binary_accuracy: 0.8846 - val_loss: -453.8604 - val_binary_accuracy: 0.8846\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -458.0807 - binary_accuracy: 0.8846 - val_loss: -470.3184 - val_binary_accuracy: 0.8846\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -474.5381 - binary_accuracy: 0.8846 - val_loss: -486.7755 - val_binary_accuracy: 0.8846\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -490.9943 - binary_accuracy: 0.8846 - val_loss: -503.2314 - val_binary_accuracy: 0.8846\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -507.4507 - binary_accuracy: 0.8846 - val_loss: -519.6869 - val_binary_accuracy: 0.8846\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -523.9064 - binary_accuracy: 0.8846 - val_loss: -536.1422 - val_binary_accuracy: 0.8846\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -540.3614 - binary_accuracy: 0.8846 - val_loss: -552.5978 - val_binary_accuracy: 0.8846\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -556.8187 - binary_accuracy: 0.8846 - val_loss: -569.0626 - val_binary_accuracy: 0.8846\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -573.2857 - binary_accuracy: 0.8846 - val_loss: -585.5349 - val_binary_accuracy: 0.8846\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -589.7595 - binary_accuracy: 0.8846 - val_loss: -602.0118 - val_binary_accuracy: 0.8846\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -606.2373 - binary_accuracy: 0.8846 - val_loss: -618.4916 - val_binary_accuracy: 0.8846\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -622.7174 - binary_accuracy: 0.8846 - val_loss: -634.9728 - val_binary_accuracy: 0.8846\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -639.1989 - binary_accuracy: 0.8846 - val_loss: -651.4547 - val_binary_accuracy: 0.8846\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -655.6805 - binary_accuracy: 0.8846 - val_loss: -667.9360 - val_binary_accuracy: 0.8846\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -672.1619 - binary_accuracy: 0.8846 - val_loss: -684.4167 - val_binary_accuracy: 0.8846\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -688.6422 - binary_accuracy: 0.8846 - val_loss: -700.8964 - val_binary_accuracy: 0.8846\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -705.1218 - binary_accuracy: 0.8846 - val_loss: -717.3749 - val_binary_accuracy: 0.8846\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -721.5997 - binary_accuracy: 0.8846 - val_loss: -733.8520 - val_binary_accuracy: 0.8846\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -738.0767 - binary_accuracy: 0.8846 - val_loss: -750.3277 - val_binary_accuracy: 0.8846\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -754.5520 - binary_accuracy: 0.8846 - val_loss: -766.8021 - val_binary_accuracy: 0.8846\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -771.0261 - binary_accuracy: 0.8846 - val_loss: -783.2748 - val_binary_accuracy: 0.8846\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: -787.4985 - binary_accuracy: 0.8846 - val_loss: -799.7462 - val_binary_accuracy: 0.8846\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -803.9694 - binary_accuracy: 0.8846 - val_loss: -816.2164 - val_binary_accuracy: 0.8846\n",
      "--> Training model class 3 vs rest\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 83ms/step - loss: 49.4664 - binary_accuracy: 0.8205 - val_loss: -10.2297 - val_binary_accuracy: 0.8462\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -14.4302 - binary_accuracy: 0.8205 - val_loss: -26.6394 - val_binary_accuracy: 0.8462\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -30.8571 - binary_accuracy: 0.8205 - val_loss: -43.0677 - val_binary_accuracy: 0.8462\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -47.2721 - binary_accuracy: 0.8205 - val_loss: -59.4926 - val_binary_accuracy: 0.8462\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -63.7048 - binary_accuracy: 0.8205 - val_loss: -75.9451 - val_binary_accuracy: 0.8462\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -80.1510 - binary_accuracy: 0.8205 - val_loss: -92.4040 - val_binary_accuracy: 0.8462\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -96.6242 - binary_accuracy: 0.8205 - val_loss: -108.8927 - val_binary_accuracy: 0.8462\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -113.1052 - binary_accuracy: 0.8205 - val_loss: -125.3838 - val_binary_accuracy: 0.8462\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -129.6100 - binary_accuracy: 0.8205 - val_loss: -141.8891 - val_binary_accuracy: 0.8462\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -146.0865 - binary_accuracy: 0.8205 - val_loss: -158.3122 - val_binary_accuracy: 0.8462\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -162.4936 - binary_accuracy: 0.8205 - val_loss: -174.6982 - val_binary_accuracy: 0.8462\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -178.8741 - binary_accuracy: 0.8205 - val_loss: -191.0508 - val_binary_accuracy: 0.8462\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -195.2019 - binary_accuracy: 0.8205 - val_loss: -207.3391 - val_binary_accuracy: 0.8462\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -211.4854 - binary_accuracy: 0.8205 - val_loss: -223.6109 - val_binary_accuracy: 0.8462\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -227.7521 - binary_accuracy: 0.8205 - val_loss: -239.8547 - val_binary_accuracy: 0.8462\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -243.9761 - binary_accuracy: 0.8205 - val_loss: -256.0433 - val_binary_accuracy: 0.8462\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -260.1685 - binary_accuracy: 0.8205 - val_loss: -272.2589 - val_binary_accuracy: 0.8462\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -276.3748 - binary_accuracy: 0.8205 - val_loss: -288.4123 - val_binary_accuracy: 0.8462\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -292.5154 - binary_accuracy: 0.8205 - val_loss: -304.5062 - val_binary_accuracy: 0.8462\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -308.5843 - binary_accuracy: 0.8205 - val_loss: -320.5185 - val_binary_accuracy: 0.8462\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -324.6080 - binary_accuracy: 0.8205 - val_loss: -336.5312 - val_binary_accuracy: 0.8462\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -340.5859 - binary_accuracy: 0.8205 - val_loss: -352.4722 - val_binary_accuracy: 0.8462\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -356.5316 - binary_accuracy: 0.8205 - val_loss: -368.4146 - val_binary_accuracy: 0.8462\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -372.4586 - binary_accuracy: 0.8205 - val_loss: -384.3294 - val_binary_accuracy: 0.8462\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -388.3861 - binary_accuracy: 0.8205 - val_loss: -400.2784 - val_binary_accuracy: 0.8462\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -404.3271 - binary_accuracy: 0.8205 - val_loss: -416.2287 - val_binary_accuracy: 0.8462\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -420.3024 - binary_accuracy: 0.8205 - val_loss: -432.2314 - val_binary_accuracy: 0.8462\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -436.3042 - binary_accuracy: 0.8205 - val_loss: -448.2216 - val_binary_accuracy: 0.8462\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -452.2754 - binary_accuracy: 0.8205 - val_loss: -464.1725 - val_binary_accuracy: 0.8462\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -468.2373 - binary_accuracy: 0.8205 - val_loss: -480.1640 - val_binary_accuracy: 0.8462\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -484.2115 - binary_accuracy: 0.8205 - val_loss: -496.0975 - val_binary_accuracy: 0.8462\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -500.1588 - binary_accuracy: 0.8205 - val_loss: -512.0522 - val_binary_accuracy: 0.8462\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -516.0911 - binary_accuracy: 0.8205 - val_loss: -527.9688 - val_binary_accuracy: 0.8462\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -532.0047 - binary_accuracy: 0.8205 - val_loss: -543.8763 - val_binary_accuracy: 0.8462\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -547.9058 - binary_accuracy: 0.8205 - val_loss: -559.7668 - val_binary_accuracy: 0.8462\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -563.8149 - binary_accuracy: 0.8205 - val_loss: -575.7147 - val_binary_accuracy: 0.8462\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -579.7740 - binary_accuracy: 0.8205 - val_loss: -591.7061 - val_binary_accuracy: 0.8462\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -595.7747 - binary_accuracy: 0.8205 - val_loss: -607.7330 - val_binary_accuracy: 0.8462\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -611.8091 - binary_accuracy: 0.8205 - val_loss: -623.7887 - val_binary_accuracy: 0.8462\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -627.8704 - binary_accuracy: 0.8205 - val_loss: -639.8674 - val_binary_accuracy: 0.8462\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -643.9539 - binary_accuracy: 0.8205 - val_loss: -655.9642 - val_binary_accuracy: 0.8462\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -660.0544 - binary_accuracy: 0.8205 - val_loss: -672.0760 - val_binary_accuracy: 0.8462\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -676.1694 - binary_accuracy: 0.8205 - val_loss: -688.2001 - val_binary_accuracy: 0.8462\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -692.2961 - binary_accuracy: 0.8205 - val_loss: -704.3344 - val_binary_accuracy: 0.8462\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -708.4329 - binary_accuracy: 0.8205 - val_loss: -720.4776 - val_binary_accuracy: 0.8462\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -724.5783 - binary_accuracy: 0.8205 - val_loss: -736.6288 - val_binary_accuracy: 0.8462\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -740.7315 - binary_accuracy: 0.8205 - val_loss: -752.7869 - val_binary_accuracy: 0.8462\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -756.8914 - binary_accuracy: 0.8205 - val_loss: -768.9512 - val_binary_accuracy: 0.8462\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -773.0575 - binary_accuracy: 0.8205 - val_loss: -785.1212 - val_binary_accuracy: 0.8462\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -789.2291 - binary_accuracy: 0.8205 - val_loss: -801.2966 - val_binary_accuracy: 0.8462\n",
      "--> Training model class 1 vs rest\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 86ms/step - loss: 39.0664 - binary_accuracy: 0.4872 - val_loss: -10.2062 - val_binary_accuracy: 0.6538\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -14.3782 - binary_accuracy: 0.6410 - val_loss: -26.3795 - val_binary_accuracy: 0.6538\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -30.4822 - binary_accuracy: 0.6410 - val_loss: -42.3990 - val_binary_accuracy: 0.6538\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -46.4744 - binary_accuracy: 0.6410 - val_loss: -58.4301 - val_binary_accuracy: 0.6538\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -62.6208 - binary_accuracy: 0.6410 - val_loss: -74.5733 - val_binary_accuracy: 0.6538\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -78.5971 - binary_accuracy: 0.6410 - val_loss: -90.4250 - val_binary_accuracy: 0.6538\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -94.5337 - binary_accuracy: 0.6410 - val_loss: -106.5012 - val_binary_accuracy: 0.6538\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -110.6700 - binary_accuracy: 0.6410 - val_loss: -122.8522 - val_binary_accuracy: 0.6538\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -127.1107 - binary_accuracy: 0.6410 - val_loss: -139.2614 - val_binary_accuracy: 0.6538\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: -143.3834 - binary_accuracy: 0.6410 - val_loss: -155.2408 - val_binary_accuracy: 0.6538\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -159.3093 - binary_accuracy: 0.6410 - val_loss: -171.1140 - val_binary_accuracy: 0.6538\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -175.2026 - binary_accuracy: 0.6410 - val_loss: -187.1398 - val_binary_accuracy: 0.6538\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -191.2588 - binary_accuracy: 0.6410 - val_loss: -203.3289 - val_binary_accuracy: 0.6538\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -207.4986 - binary_accuracy: 0.6410 - val_loss: -219.6305 - val_binary_accuracy: 0.6538\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -223.7942 - binary_accuracy: 0.6410 - val_loss: -235.9473 - val_binary_accuracy: 0.6538\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -240.0945 - binary_accuracy: 0.6410 - val_loss: -252.1058 - val_binary_accuracy: 0.6538\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -256.1091 - binary_accuracy: 0.6410 - val_loss: -267.9154 - val_binary_accuracy: 0.6538\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -271.9852 - binary_accuracy: 0.6410 - val_loss: -283.8425 - val_binary_accuracy: 0.6538\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -287.8577 - binary_accuracy: 0.6410 - val_loss: -299.4513 - val_binary_accuracy: 0.6538\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -303.3050 - binary_accuracy: 0.6410 - val_loss: -314.6693 - val_binary_accuracy: 0.6538\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -318.6920 - binary_accuracy: 0.6410 - val_loss: -330.3942 - val_binary_accuracy: 0.6538\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -334.4203 - binary_accuracy: 0.6410 - val_loss: -346.3183 - val_binary_accuracy: 0.6538\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -350.5449 - binary_accuracy: 0.6410 - val_loss: -362.5428 - val_binary_accuracy: 0.6538\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -366.5047 - binary_accuracy: 0.6410 - val_loss: -378.1284 - val_binary_accuracy: 0.6538\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -382.1246 - binary_accuracy: 0.6410 - val_loss: -393.6996 - val_binary_accuracy: 0.6538\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -397.5311 - binary_accuracy: 0.6410 - val_loss: -408.8219 - val_binary_accuracy: 0.6538\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -412.6872 - binary_accuracy: 0.6410 - val_loss: -424.1408 - val_binary_accuracy: 0.6538\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -428.0135 - binary_accuracy: 0.6410 - val_loss: -439.4831 - val_binary_accuracy: 0.6538\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -443.4804 - binary_accuracy: 0.6410 - val_loss: -455.1552 - val_binary_accuracy: 0.6538\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -459.1708 - binary_accuracy: 0.6410 - val_loss: -470.9493 - val_binary_accuracy: 0.6538\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -475.0504 - binary_accuracy: 0.6410 - val_loss: -486.7985 - val_binary_accuracy: 0.6538\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -490.8184 - binary_accuracy: 0.6410 - val_loss: -502.4558 - val_binary_accuracy: 0.6538\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -506.2977 - binary_accuracy: 0.6410 - val_loss: -517.8640 - val_binary_accuracy: 0.6538\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -521.9551 - binary_accuracy: 0.6410 - val_loss: -533.7430 - val_binary_accuracy: 0.6538\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -537.7631 - binary_accuracy: 0.6410 - val_loss: -549.4437 - val_binary_accuracy: 0.6538\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -553.4983 - binary_accuracy: 0.6410 - val_loss: -565.2065 - val_binary_accuracy: 0.6538\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -569.2463 - binary_accuracy: 0.6410 - val_loss: -580.9880 - val_binary_accuracy: 0.6538\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -585.0261 - binary_accuracy: 0.6410 - val_loss: -596.8170 - val_binary_accuracy: 0.6538\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -600.9011 - binary_accuracy: 0.6410 - val_loss: -612.8044 - val_binary_accuracy: 0.6538\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -616.9244 - binary_accuracy: 0.6410 - val_loss: -628.9219 - val_binary_accuracy: 0.6538\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -633.0648 - binary_accuracy: 0.6410 - val_loss: -645.1290 - val_binary_accuracy: 0.6538\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -649.2935 - binary_accuracy: 0.6410 - val_loss: -661.4145 - val_binary_accuracy: 0.6538\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -665.5951 - binary_accuracy: 0.6410 - val_loss: -677.7593 - val_binary_accuracy: 0.6538\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -681.9512 - binary_accuracy: 0.6410 - val_loss: -694.1494 - val_binary_accuracy: 0.6538\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -698.3517 - binary_accuracy: 0.6410 - val_loss: -710.5770 - val_binary_accuracy: 0.6538\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -714.7854 - binary_accuracy: 0.6410 - val_loss: -727.0312 - val_binary_accuracy: 0.6538\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -731.2462 - binary_accuracy: 0.6410 - val_loss: -743.5066 - val_binary_accuracy: 0.6538\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -747.7253 - binary_accuracy: 0.6410 - val_loss: -759.9945 - val_binary_accuracy: 0.6538\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -764.2158 - binary_accuracy: 0.6410 - val_loss: -776.4911 - val_binary_accuracy: 0.6538\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -780.7140 - binary_accuracy: 0.6410 - val_loss: -792.9927 - val_binary_accuracy: 0.6538\n",
      "--> Training model class 0 vs rest\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 86ms/step - loss: 13.2586 - binary_accuracy: 0.8974 - val_loss: -10.2457 - val_binary_accuracy: 0.8846\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -14.4050 - binary_accuracy: 0.9103 - val_loss: -26.4014 - val_binary_accuracy: 0.8846\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: -30.5204 - binary_accuracy: 0.9103 - val_loss: -42.4107 - val_binary_accuracy: 0.8846\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -46.5087 - binary_accuracy: 0.9103 - val_loss: -58.2661 - val_binary_accuracy: 0.8846\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -62.3101 - binary_accuracy: 0.9103 - val_loss: -73.9459 - val_binary_accuracy: 0.8846\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -77.9632 - binary_accuracy: 0.9103 - val_loss: -89.5588 - val_binary_accuracy: 0.8846\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -93.5885 - binary_accuracy: 0.9103 - val_loss: -105.1800 - val_binary_accuracy: 0.8846\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -109.2084 - binary_accuracy: 0.9103 - val_loss: -120.7903 - val_binary_accuracy: 0.8846\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -124.8134 - binary_accuracy: 0.9103 - val_loss: -136.4913 - val_binary_accuracy: 0.8846\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -140.6153 - binary_accuracy: 0.9103 - val_loss: -152.5338 - val_binary_accuracy: 0.8846\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -156.7787 - binary_accuracy: 0.9103 - val_loss: -169.0154 - val_binary_accuracy: 0.8846\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -173.3694 - binary_accuracy: 0.9103 - val_loss: -185.8802 - val_binary_accuracy: 0.8846\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -190.3204 - binary_accuracy: 0.9103 - val_loss: -203.1003 - val_binary_accuracy: 0.8846\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -207.6758 - binary_accuracy: 0.9103 - val_loss: -220.7923 - val_binary_accuracy: 0.8846\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -225.4869 - binary_accuracy: 0.9103 - val_loss: -238.8865 - val_binary_accuracy: 0.8846\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -243.5741 - binary_accuracy: 0.9103 - val_loss: -257.0333 - val_binary_accuracy: 0.8846\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -261.7815 - binary_accuracy: 0.9103 - val_loss: -275.3892 - val_binary_accuracy: 0.8846\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -280.1983 - binary_accuracy: 0.9103 - val_loss: -293.8235 - val_binary_accuracy: 0.8846\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -298.6289 - binary_accuracy: 0.9103 - val_loss: -312.2611 - val_binary_accuracy: 0.8846\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -317.0392 - binary_accuracy: 0.9103 - val_loss: -330.6259 - val_binary_accuracy: 0.8846\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -335.3652 - binary_accuracy: 0.9103 - val_loss: -348.8642 - val_binary_accuracy: 0.8846\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: -353.6247 - binary_accuracy: 0.9103 - val_loss: -367.0591 - val_binary_accuracy: 0.8846\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -371.7381 - binary_accuracy: 0.9103 - val_loss: -385.0175 - val_binary_accuracy: 0.8846\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -389.7107 - binary_accuracy: 0.9103 - val_loss: -402.9553 - val_binary_accuracy: 0.8846\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -407.6313 - binary_accuracy: 0.9103 - val_loss: -420.7716 - val_binary_accuracy: 0.8846\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -425.3597 - binary_accuracy: 0.9103 - val_loss: -438.3662 - val_binary_accuracy: 0.8846\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -442.9595 - binary_accuracy: 0.9103 - val_loss: -455.9383 - val_binary_accuracy: 0.8846\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -460.5146 - binary_accuracy: 0.9103 - val_loss: -473.4613 - val_binary_accuracy: 0.8846\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -478.0087 - binary_accuracy: 0.9103 - val_loss: -490.9489 - val_binary_accuracy: 0.8846\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -495.5398 - binary_accuracy: 0.9103 - val_loss: -508.5072 - val_binary_accuracy: 0.8846\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -513.0777 - binary_accuracy: 0.9103 - val_loss: -526.0074 - val_binary_accuracy: 0.8846\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -530.5400 - binary_accuracy: 0.9103 - val_loss: -543.3842 - val_binary_accuracy: 0.8846\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -547.8832 - binary_accuracy: 0.9103 - val_loss: -560.5934 - val_binary_accuracy: 0.8846\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -565.0461 - binary_accuracy: 0.9103 - val_loss: -577.6290 - val_binary_accuracy: 0.8846\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -582.0443 - binary_accuracy: 0.9103 - val_loss: -594.5335 - val_binary_accuracy: 0.8846\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -598.9235 - binary_accuracy: 0.9103 - val_loss: -611.3453 - val_binary_accuracy: 0.8846\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -615.7169 - binary_accuracy: 0.9103 - val_loss: -628.0896 - val_binary_accuracy: 0.8846\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -632.4475 - binary_accuracy: 0.9103 - val_loss: -644.7822 - val_binary_accuracy: 0.8846\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -649.1313 - binary_accuracy: 0.9103 - val_loss: -661.4379 - val_binary_accuracy: 0.8846\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: -665.7802 - binary_accuracy: 0.9103 - val_loss: -678.0671 - val_binary_accuracy: 0.8846\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -682.4017 - binary_accuracy: 0.9103 - val_loss: -694.6746 - val_binary_accuracy: 0.8846\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -699.0021 - binary_accuracy: 0.9103 - val_loss: -711.2623 - val_binary_accuracy: 0.8846\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -715.5848 - binary_accuracy: 0.9103 - val_loss: -727.8400 - val_binary_accuracy: 0.8846\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -732.1575 - binary_accuracy: 0.9103 - val_loss: -744.4026 - val_binary_accuracy: 0.8846\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -748.7172 - binary_accuracy: 0.9103 - val_loss: -760.9551 - val_binary_accuracy: 0.8846\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -765.2690 - binary_accuracy: 0.9103 - val_loss: -777.4993 - val_binary_accuracy: 0.8846\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -781.8132 - binary_accuracy: 0.9103 - val_loss: -794.0352 - val_binary_accuracy: 0.8846\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -798.3510 - binary_accuracy: 0.9103 - val_loss: -810.5661 - val_binary_accuracy: 0.8846\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: -814.8834 - binary_accuracy: 0.9103 - val_loss: -827.0964 - val_binary_accuracy: 0.8846\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: -831.4108 - binary_accuracy: 0.9103 - val_loss: -843.6246 - val_binary_accuracy: 0.8846\n"
     ]
    }
   ],
   "source": [
    "model3 = MultiFADLModelOvR(\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_val_scaled,\n",
    "    y_val,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "for clas, model in model3.models.items():\n",
    "    model.model.save(f'results/brain_GSE50161/brain_GSE50161_MultiFADLModel_{clas}.h5')    \n",
    "    with open(f'results/brain_GSE50161/brain_GSE50161_MultiFADLModel_history_{clas}.pkl', 'wb') as f:\n",
    "        pickle.dump(model.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Evaluating model class 2 vs rest\n",
      "1/1 [==============================] - 0s 19ms/step - loss: -794.6967 - binary_accuracy: 0.7308\n",
      "{'loss': -794.6966552734375, 'accuracy': 0.7307692170143127, 'f1': 0.6170940170940169}\n",
      "--> Evaluating model class 4 vs rest\n",
      "1/1 [==============================] - 0s 17ms/step - loss: -816.2164 - binary_accuracy: 0.8846\n",
      "{'loss': -816.2163696289062, 'accuracy': 0.8846153616905212, 'f1': 0.8304552590266875}\n",
      "--> Evaluating model class 3 vs rest\n",
      "1/1 [==============================] - 0s 17ms/step - loss: -801.2966 - binary_accuracy: 0.8462\n",
      "{'loss': -801.2965698242188, 'accuracy': 0.8461538553237915, 'f1': 0.7756410256410255}\n",
      "--> Evaluating model class 1 vs rest\n",
      "1/1 [==============================] - 0s 16ms/step - loss: -792.9927 - binary_accuracy: 0.6538\n",
      "{'loss': -792.9927368164062, 'accuracy': 0.6538461446762085, 'f1': 0.5169946332737031}\n",
      "--> Evaluating model class 0 vs rest\n",
      "1/1 [==============================] - 0s 17ms/step - loss: -843.7061 - binary_accuracy: 0.8846\n",
      "{'loss': -843.7061157226562, 'accuracy': 0.8846153616905212, 'f1': 0.8304552590266875}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>number of selected features</th>\n",
       "      <th>selected Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NoSelection</th>\n",
       "      <td>1.51139</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.178022</td>\n",
       "      <td>54675</td>\n",
       "      <td>[1007_s_at, 1053_at, 117_at, 121_at, 1255_g_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonoFADL</th>\n",
       "      <td>-815.504</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.178022</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiFADL</th>\n",
       "      <td>{2: -794.6966552734375, 4: -816.2163696289062,...</td>\n",
       "      <td>{2: 0.7307692170143127, 4: 0.8846153616905212,...</td>\n",
       "      <td>{2: 0.6170940170940169, 4: 0.8304552590266875,...</td>\n",
       "      <td>{2: 0, 4: 0, 3: 0, 1: 0, 0: 1, 'global': 1}</td>\n",
       "      <td>{2: [], 4: [], 3: [], 1: [], 0: ['212533_at'],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     test_loss  \\\n",
       "NoSelection                                            1.51139   \n",
       "MonoFADL                                              -815.504   \n",
       "MultiFADL    {2: -794.6966552734375, 4: -816.2163696289062,...   \n",
       "\n",
       "                                                 test_accuracy  \\\n",
       "NoSelection                                           0.346154   \n",
       "MonoFADL                                              0.346154   \n",
       "MultiFADL    {2: 0.7307692170143127, 4: 0.8846153616905212,...   \n",
       "\n",
       "                                                       test_f1  \\\n",
       "NoSelection                                           0.178022   \n",
       "MonoFADL                                              0.178022   \n",
       "MultiFADL    {2: 0.6170940170940169, 4: 0.8304552590266875,...   \n",
       "\n",
       "                             number of selected features  \\\n",
       "NoSelection                                        54675   \n",
       "MonoFADL                                               0   \n",
       "MultiFADL    {2: 0, 4: 0, 3: 0, 1: 0, 0: 1, 'global': 1}   \n",
       "\n",
       "                                             selected Features  \n",
       "NoSelection  [1007_s_at, 1053_at, 117_at, 121_at, 1255_g_at...  \n",
       "MonoFADL                                                    []  \n",
       "MultiFADL    {2: [], 4: [], 3: [], 1: [], 0: ['212533_at'],...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "verbose3 = model3.get_verbose()\n",
    "\n",
    "comparative_results.loc['MultiFADL'] = [verbose3['results']['loss'], \n",
    "                                          verbose3['results']['accuracy'], \n",
    "                                          verbose3['results']['f1'], \n",
    "                                          {clas: verbose3['selected_features'][clas].shape[0] for clas in verbose3['selected_features']}, \n",
    "                                          verbose3['selected_features']]\n",
    "\n",
    "comparative_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_results.to_csv('results/brain_GSE50161/brain_ComparativeResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
